---
title: "CLV-QUANTILE test"
output: html_notebook
---

Info
====

cdata/CLV quantile vs Spark approxQuantile vs Hive percentile* precision


References
==========

Definition
-----

### Stats

 * [Wikipedia: Quantile Estimation](https://en.wikipedia.org/wiki/Quantile#Estimating_quantiles_from_a_sample)
 * [R quantile](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/quantile.html)
 * [Wolfram MathWorld](http://mathworld.wolfram.com/Quantile.html)
 * [SAS blogs](https://blogs.sas.com/content/iml/2017/05/24/definitions-sample-quantiles.html)


### Spark

 * [API doc](http://spark.apache.org/docs/latest/api/R/approxQuantile.html)
 * [Databricks Blogs](https://databricks.com/blog/2016/05/19/approximate-algorithms-in-apache-spark-hyperloglog-and-quantiles.html)
 * [Article](http://infolab.stanford.edu/~datar/courses/cs361a/papers/quantiles.pdf)
 

### Hive

 * [API doc](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF)
 


Pre-Requisites
==============


CRAN
----

```{r}
library(readr)
library(dplyr)
library(ggplot2)

```



SPARK
=====

Spark Shell
-----------

Example of _Spark_ *approxQuantile* and _Hive_ *percentile* invocation 

```{r}
Comment <- function(`@Comments`) {invisible()}

q_test_spark <- function() {
  # spark-shell
  Comment(`

          import org.apache.spark.sql._
          import org.apache.spark.sql.functions._
          import scala.io.Source
          import spark.implicits._

          val dataFile = "./data/clv_sample_orders.csv"
          val dateFrom = "2015-12-13"
          val dateTo = "2015-12-13"
          
          val paramsQuantiles = Seq(0.1, 0.25, 0.5, 0.85, 0.9)

          val ds = Source.fromFile(fn)
          .getLines.drop(1).toList.map(_.split(",")).map(x => (x(0), x(3), x(2).toDouble))
          .toDS.cache
          ds.show

          val tst: Dataset[Double] = ds
          .filter(r => r._2 >= dateFrom && r._2 < dateTo)
          .groupBy("_1").agg(sum("_3").alias("s")).select("s").filter("s > 0").as[Double]

          println(tst.count)
          println(tst.groupBy(lit(1)).agg(sum("s")).collect.apply(0))

          val ttt = tst
          // val ttt = tst.union(tst).union(tst).union(tst).union(tst).union(tst).union(tst)
          //     .union(tst).union(tst).union(tst).union(tst).union(tst).union(tst)

          val q = ttt.stat.approxQuantile("s", paramsQuantiles.toArray, 0.0)

          println("\n\n\nQUANTILES => \n\n\n qk.approx <- "+(q.toList.toString).replaceAll("List\\(","c\\(")+" \n\n\n")


          val tempTable = s"tst_tst"
          ttt.createOrReplaceTempView(tempTable)
          //val str = para
          spark.sqlContext.sql("select percentile_approx(s, 0.9) from tst_tst").show
          spark.sqlContext.sql("select percentile(s, 0.9) from tst_tst").show
          spark.catalog.dropTempView(tempTable)


  `)

}
```


CLV Test
========

Definition
----------

```{r}
q_config <- function() {
  
  cnf <- new.env()
  
  cnf$input_data_base <- c(".","./data", "../data", "../../data") # set current directory
  cnf$input_data_file <- "clv_sample_orders.csv"

  cnf$date.min <- as.Date('2015-12-13')
  cnf$date.max <- as.Date('2017-12-13')

  cnf$qp.inner <- c(0.1, 0.25, 0.5, 0.85, 0.9)
  
  cnf$qp <- c(0, cnf$qp.inner, 1.0) # seq(0,1, by=0.25)
  cnf$qt <- c(1:9) # R quantile type
  
  config <<- cnf
  
  return (cnf)
}
cnf <- q_config()
```

```{r}
q_find_file <- function(name, cnf = config) {
  
  base <- cnf$input_data_base
  
  for (p in base){
    fn <- paste(p, name, sep = "/")
      cat("+++ ??? check file: ", fn, " in ", getwd(), "\n")
    
    if (file.exists(fn)) {
      cat("+++ found input data file: ", fn, "\n")
      return (fn)
    }
  }  
  stop(paste("input data not found:", name, "in", base))
}
```




```{r}
clv_load_data <- function(name, cnf = config) {
  
  fn <- q_find_file(name, cnf)

  clv_sample_orders <- read_csv(fn, col_types = cols(
    date = col_date(format = "%Y-%m-%d"),
    raw_date = col_date(format = "%Y-%m-%d")))

  #View(clv_sample_orders)

  dt.a <- cnf$date.min
  dt.b <- cnf$date.max

  clv_sales.z <- clv_sample_orders %>%
    filter((raw_date >= dt.a)) %>%
    filter((raw_date < dt.b)) %>%
    group_by(customer_id) %>%
    summarize(total=(sum(sales)))

  clv_sales <<- clv_sales.z[clv_sales.z$total > 0,]

  return(clv_sales)
    
}

```


```{r}
q_verify <- function(x, qk, qp, qt, cnf = config) {
  
  res <- c()
  res$dm <- c()
  
  res$x <- x
  res$dm$n <- length(x)
  
  res$dm$qk <- qk   # spark quantiles (with 0.0 and 1.0 values)
  res$dm$qp <- qp   # quantile prob level to consider
  res$dm$qt <- qt   # R quantile types

  norm_vec <- function(x) sqrt(sum(x^2))

  # compute r quantiles for all supported types  
  qm <- matrix(nrow=length(qt), ncol=length(qp))
  for (t in qt) {
    qm[t,] <- quantile(x, probs = qp, type = t)
  }
  res$dm$qm <- qm
  
  q7 <- quantile(x, probs = qp)
  res$dm$q7 <- q7 # R default method
  
  # find best match with min norm(diff)
  qr <- matrix(nrow=length(qt), ncol=length(qp))
  qu <- matrix(nrow=length(qt), ncol=length(qp))
  qe <- 0.0
  for (t in qt) {
    qr[t,] <- qm[t,] - qk
    qu[t,] <- qr[t,]/qm[t,]*100.0
    qe[t] <- norm_vec(qr[t,])  # norm_2 diffference
  }
  res$dm$qr <- qr
  res$dm$qu <- qu
  res$dm$qe <- qe
  
  qi.e <- which.min(qe)     # best match (norm)  with min difference
  res$dm$qi.e <- qi.e

  res$dm$o.e.qm <- qm[qi.e,]   # best match among all R quntiles
  res$dm$o.e.qr <- qr[qi.e,]   # difference with best match
  
  
  qs <- qm %*% qk           # scalar product

  qi.s <- which.max(qs)     # best match (scalar prod) with max alignment
  
  res$dm$qs <- qs
  res$dm$qi.s <- qi.s
  
  
  qi <- qi.e      # use norm for best match
  ql <- qm[qi,]
  qd <- qk -ql
  
  res$dm$qi <- qi  # index of best match 
  res$dm$ql <- ql  # values of best match 
  res$dm$qd <- qd  # difference of best match 
  

  #q <- ql   # best match
  q <- q7    # default type
  
  res$dm$q <- q 
  return(res)

}
```


```{r}
q_bin <- function(x, q, qk, qp, cnf = config) {
  
  res <- c()
  res$dm <- c()
  
  # res$x <- x
  n <- length(x)
  res$n <- n
  
  res$dm$q <- q     # R computed quantile types
  res$dm$qk <- qk   # spark quantiles (with 0.0 and 1.0 values)
  res$dm$qp <- qp   # quantile prob level to consider

  norm_vec <- function(x) sqrt(sum(x^2))
  
  df <- data.frame("id"= names(x), x, row.names = "id")
  
  df$clazz <- cut(df$x, breaks=q,include.lowest=TRUE)       # class by R quantiles
  df$clazk <- cut(df$x, breaks=qk,include.lowest=TRUE)
#  df$iclazz <- as.numeric(as.character(df$clazz))
#  df$iclazk <- as.numeric(as.character(df$clazk))
#  df$diff <- df$iclazz - df$iclazk

  df$diff <- as.numeric(df$clazz) - as.numeric(df$clazk)

  df.diff <- df[ df$diff != 0,]

  dz <- df %>% count(clazz)
  dk <- df %>% count(clazk)


  dz$bin <- seq.int(nrow(dz))
  dk$bin <- seq.int(nrow(dk))
  
  dz$z.pq <-dz$n / n
  dk$k.pq <-dk$n / n

  dc <- dz %>% inner_join(dk, by=c("bin"))
  
  dc$n.d <- dc$n.x - dc$n.y
  dc$d.pq <-dc$n.d / n

  res$df <- df

  res$dz <- dz
  res$dk <- dk

  res$dc <- dc
  

  full_count <- nrow(df)
  diff_count <- nrow(df.diff)
  diff_rel <- diff_count / full_count
  diff_perc <- diff_rel * 100


  res$dm$full_count <- full_count
  res$dm$diff_count <- diff_count
  res$dm$diff_rel <- diff_rel
  res$dm$diff_perc <- diff_perc

  res$df.diff <- df.diff
  
  return(res)

}
```



Case:1 - #755 distinct customers
-------------------------------

### Load CLV Data

  * file: `r config$input_data_file`
  * time range: [2015-12-13..2017-12-13)



```{r}
q_clv_test_1_load <- function(cnf = config) {

  res <- c()
  res$dm <- c()
  
  name <- cnf$input_data_file
  
  clv_sales <- clv_load_data(name,cnf)

  sale.min <- min(clv_sales$total)
  sale.max <- max(clv_sales$total)
  
  x <- clv_sales$total
  names(x) <- clv_sales$customer_id
  

  res$case.name <- name
  res$clv_sales <- clv_sales
  res$dm$n <- nrow(clv_sales)
  res$dm$sale.min <- sale.min
  res$dm$sale.max <- sale.max

  return (res)
}
res.1.in <<- q_clv_test_1_load()
```

#### test1.load:

  * num.customers: `r res.1.in$dm$n`


```{r}

  res.1.in$dm
  
```

data value summary:

```{r}

   summary(res.1.in$clv_sales$total)
  
```
  

### Quantiles with Spark.approxQuantile

  * probs: `r config$qp`


```{r}
q_clv_test_1x <- function(cnf = config) {

  res <- c()
  res$dm <- c()
  
  sale.min <- res.1.in$dm$sale.min
  sale.max <- res.1.in$dm$sale.max
  
  x <- res.1.in$x
  

  # spark approxQuantile => List(132.537313432836, 261.919559619995, 579.17, 2967.458968127149, 4316.64192111549)
  
  # => see: q_test_spark()
  
  qk.approx <- c(132.537313432836, 261.919559619995, 579.17, 2967.458968127149, 4316.64192111549)
  qk <- c(sale.min, qk.approx, sale.max)
  
  qp <- cnf$qp
  qt <- cnf$qt
  
  
  res$dm$qk <- qk
  res$dm$qp <- qp
  res$dm$qt <- qt

  return (res)

}
res.1.x <<- q_clv_test_1x()
```

#### test1.approx:

```{r}

  res.1.x$dm
  
```


### R Quantile Types (1..9)

'quantile' default method: Type = 7

```{r}
q_clv_test_1a <- function(cnf = config) {

  name <- cnf$input_data_file
  
  clv_sales <<- clv_load_data(name,cnf)

  sale.min <- min(clv_sales$total)
  sale.max <- max(clv_sales$total)
  
  x <<- clv_sales$total
  names(x) <- clv_sales$customer_id
  

  # spark approxQuantile => List(132.537313432836, 261.919559619995, 579.17, 2967.458968127149, 4316.64192111549)
  qk.approx <<- c(132.537313432836, 261.919559619995, 579.17, 2967.458968127149, 4316.64192111549)
  qk <<- c(sale.min, qk.approx, sale.max)
  
  qp <- cnf$qp
  qt <- cnf$qt
  
  res <- q_verify(x, qk, qp, qt)
  
  return (res)
}
res.1.a <<- q_clv_test_1a()
```

### test1.verify: (#755)

```{r}

  res.1.a$dm
  
```


### Classification Difference


```{r}
q_clv_test_1b <- function(cnf = config) {

  x <- res.1.a$x
  q <- res.1.a$dm$q
  qk <- res.1.a$dm$qk
  qp <- res.1.a$dm$qp

  res <- q_bin(x, q, qk, qp)
  
  return(res)

}
res.1.b <<- q_clv_test_1b()
```

#### test1.summary:

```{r}

  res.1.b$dm
  
```

#### test1.classes:

```{r}

  res.1.b$dc
  
```


#### test1.details:


```{r}
res.1.b$df.diff
```



